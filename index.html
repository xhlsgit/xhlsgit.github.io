<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 35px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/favicon_io/favicon.ico">
  <title>Xinhu Li</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Xinhu Li</name>
              </p>
              <!-- <p>I am a Research Scientist at Meta. I completed my PhD in Computer Science from <a href="https://clvrai.com">Cognitive Learning for Vision and Robotics Lab (CLVR)</a>, at University of Southern California, advised by Prof. <a href="https://clvrai.com/web_lim/">Joseph J. Lim</a>. My research interests include Reinforcement Learning, Deep Learning, and Robotics. -->
              <p>I am an intern at <a href="https://www.usc.edu/">USC</a>, advised by <a href="https://ebiyik.github.io/">Erdem B&#305;y&#305;k</a>. Prior to this experience, I was advised by <a href="https://clvrai.com/web_lim/">Joseph J. Lim</a>.
                I received my master's degree in Computer Science from <a href="https://www.usc.edu/">USC</a>. Before that, I completed my B.Eng. in Software Engineering at the Zhejiang University of Technology.
              </p>
              <p>
              <strong>Research Goal</strong>: My overarching research ambition is to create robust, versatile agents capable of navigating and resolving tasks in unstructured environments. To this end, I am drawn to several key areas:
              Robotics, Reinforcement Learning, Machine Learning
              </p>
              <p align=center>
                <a href="mailto:lixinhu98@gmail.com">Email</a> &nbsp|&nbsp
                <!-- <a href="https://twitter.com/Ayushj240">Twitter</a> &nbsp|&nbsp -->
                <a href="data/Xinhu_s_CV.pdf">CV</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=hXXnM74AAAAJ&hl=en">Google Scholar</a>
                <!-- <a href="https://www.linkedin.com/in/ayushj240"> LinkedIn </a> -->
              </p>
            </td>
            <td width="33%">
              <img src="images/profile.png" width="200">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
            </td>
          </tr>
        </table>

        <!-- papers -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="30%">
           <a href="">
             <img src="images/constrained_teaser.png" href="" alt="GRIP Teaser" width="300" height="120" type="image/png"></td>
           </a>
           <td valign="middle" width="70%">
             <a href="">
               <papertitle>When a Robot is More Capable than a Human: Learning from Constrained Demonstrators</papertitle>
             </a>
             <br>
             <strong>Xinhu Li</strong>,
             <a href="https://ayushj240.github.io/">Ayush Jain</a>,
             <a href="https://yang-zj1026.github.io/">Zhaojing Yang</a>,
             <a href="https://ygtkorkmaz.github.io/">Yigit Korkmaz</a>,
             <!-- <a href="">Mengchen Lin</a>,  -->
             <a href="https://ebiyik.github.io/">Erdem B&#305;y&#305;k</a>
             <br>
             <em>In Submission</em>
             <br>
             <p>We address learning from constrained demonstrators by propagating goal proximity rewards to out-of-distribution 
              states via confidence-based filtering and interpolation.</p>
             <!-- <a href="https://openreview.net/forum?id=H3jcTxcvvJ">Paper</a> |
             <a href="https://arxiv.org/abs/2410.11833">arXiv</a> -->
             <p></p>
           </td>

       </table>




        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="30%">
           <a href="https://arxiv.org/abs/2410.11833">
             <img src="images/Successive_AC_teaser.png" href="https://arxiv.org/abs/2410.11833" alt="SAVO Teaser" width="300" height="120" type="image/png"></td>
           </a>
           <td valign="middle" width="70%">
             <a href="https://arxiv.org/abs/2410.11833">
               <papertitle>Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions</papertitle>
             </a>
             <br>
             <a href="https://ayushj240.github.io/">Ayush Jain</a>,
             <a href="https://rowing0914.github.io/">Norio Kosaka</a>,
             <strong>Xinhu Li</strong>,
             <a href="https://bi.snu.ac.kr/~kmkim/">Kyung-Min Kim</a>,
             <a href="https://ebiyik.github.io/">Erdem B&#305;y&#305;k</a>,
             <a href="https://clvrai.com/web_lim/">Joseph J. Lim</a>
             <br>
             <em>RLC 2025, Reinforcement Learning Conference</em>
             <br>
              <a href="https://rl-conference.cc/RLC2025Awards.html" style="color:red; font-style:italic; font-weight:bold;">
                Outstanding Paper Award on Empirical Reinforcement Learning Research
              </a>
             <p>We identify that TD3 gets stuck in local optima in tasks with complex Q-functions and propose a new actor architecture to find better optima.</p>
             <a href="https://openreview.net/forum?id=H3jcTxcvvJ">Paper</a> |
             <a href="https://arxiv.org/abs/2410.11833">arXiv</a>
             <p></p>
           </td>

       </table>




       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <td width="30%">
         <a href="">
           <img src="images/reward_adaptation.png" href="" alt="SSL Teaser" width="300" height="120" type="image/png"></td>
         </a>
         <td valign="middle" width="70%">
           <a href="">
             <papertitle>Beyond Policy Transfer: Self-Supervised Reward Adaptation</papertitle>
           </a>
           <br>
           <strong>Xinhu Li*</strong>,
           <a href="https://ayushj240.github.io/">Ayush Jain*</a>,
           <a href="https://clvrai.com/web_lim/">Joseph J. Lim</a>,
           <a href="https://ebiyik.github.io/">Erdem B&#305;y&#305;k</a>
           <br>
           <em>In Progress</em>
           <br>
            <!-- <a href="https://rl-conference.cc/RLC2025Awards.html" style="color:red; font-style:italic; font-weight:bold;">
              Outstanding Paper Award on Empirical Reinforcement Learning Research
            </a> -->
           <p>we introduce a self-supervised reward adaptation method for adapting policies without human assistance. It 
            Exceeds all other adaptation methods in manipulation and locomotion environment adaptation.</p>
           <!-- <a href="https://openreview.net/forum?id=H3jcTxcvvJ">Paper</a> |
           <a href="https://arxiv.org/abs/2410.11833">arXiv</a> -->
           <p></p>
         </td>

     </table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  Credits to the
                  <a href="https://jonbarron.info/">
                    <font size="2">
                      Coolest template!
                    </font>
                  </a>
                </font>
              </p>
            </td>
          </tr>
        </table>



        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
